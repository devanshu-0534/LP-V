
# TensorFlow and tf.keras
import tensorflow as tf

# Helper libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print(tf.__version__)











train_df = pd.read_csv('fashion-mnist_train.csv')
test_df = pd.read_csv('fashion-mnist_test.csv')


fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()













class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']








train_images.shape










len(train_labels)





train_labels







test_images.shape








len(test_labels)







plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()





# normalize the data

train_images = train_images / 255.0

test_images = test_images / 255.0









plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()











model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])








model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])








model.fit(train_images, train_labels, epochs=30)








test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)

print('\nTest accuracy:', test_acc)











probability_model = tf.keras.Sequential([model,
                                         tf.keras.layers.Softmax()])






predictions = probability_model.predict(test_images)








predictions[0]









np.argmax(predictions[0])










test_labels[0]










def plot_image(i, predictions_array, true_label, img):
  true_label, img = true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img, cmap=plt.cm.binary)

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
  true_label = true_label[i]
  plt.grid(False)
  plt.xticks(range(10))
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')












i = 0
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()










i = 12
plt.figure(figsize=(6,3))
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i],  test_labels)
plt.show()












# Plot the first X test images, their predicted labels, and the true labels.
# Color correct predictions in blue and incorrect predictions in red.
num_rows = 5
num_cols = 3
num_images = num_rows*num_cols
plt.figure(figsize=(2*2*num_cols, 2*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_image(i, predictions[i], test_labels, test_images)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_value_array(i, predictions[i], test_labels)
plt.tight_layout()
plt.show()












# Grab an image from the test dataset.
img = test_images[1]

print(img.shape)



















# Add the image to a batch where it's the only member.
img = (np.expand_dims(img,0))

print(img.shape)












predictions_single = probability_model.predict(img)

print(predictions_single)










plot_value_array(1, predictions_single[0], test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)
plt.show()







np.argmax(predictions_single[0])
















=---------------------

Detailed Code Explanation
Step 1: Import Libraries
python
Copy
Edit
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print(tf.__version__)
➡️ Libraries used:

TensorFlow/Keras — Deep Learning framework

NumPy — Numerical operations

Pandas — For dataframes (though unused later)

Matplotlib — Plotting

Step 2: Load Dataset
python
Copy
Edit
train_df = pd.read_csv('fashion-mnist_train.csv')
test_df = pd.read_csv('fashion-mnist_test.csv')
➡️ Reads Fashion MNIST CSVs (but note: rest of code doesn't use these)

Preferred approach (used later):

python
Copy
Edit
fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
➡️ Loads Fashion MNIST:

Images = 28x28 grayscale

Labels = integer class labels (0-9)

Step 3: Class Names
python
Copy
Edit
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
➡️ Maps numeric labels → human-readable classes

Step 4: Explore Data
python
Copy
Edit
train_images.shape  # (60000, 28, 28)
len(train_labels)   # 60000
test_images.shape   # (10000, 28, 28)
len(test_labels)    # 10000
➡️ Training set: 60,000 images
➡️ Test set: 10,000 images

Step 5: Visualize one image
python
Copy
Edit
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()
➡️ Plots 1st training image

Step 6: Normalize pixel values
python
Copy
Edit
train_images = train_images / 255.0
test_images = test_images / 255.0
➡️ Rescales pixel values [0,255] → [0,1]
Helps training faster and more stable

Step 7: Plot first 25 images with labels
python
Copy
Edit
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()
➡️ Shows 25 training images + their class labels

Step 8: Build Neural Network
python
Copy
Edit
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])
➡️ Architecture:

Flatten: 28×28 → 784 (1D)

Dense layer: 128 neurons + ReLU

Output Dense: 10 neurons (class logits)

Step 9: Compile model
python
Copy
Edit
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
➡️ Adam optimizer
➡️ SparseCategoricalCrossentropy: Because labels are integers (not one-hot)

Step 10: Train Model
python
Copy
Edit
model.fit(train_images, train_labels, epochs=30)
➡️ Trains for 30 epochs
➡️ No explicit validation set (could be improved)

Step 11: Evaluate on Test Data
python
Copy
Edit
test_loss, test_acc = model.evaluate(test_images, test_labels)
print('\nTest accuracy:', test_acc)
➡️ Returns loss and accuracy on test set

Step 12: Make Predictions (Convert logits → probabilities)
python
Copy
Edit
probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])
predictions = probability_model.predict(test_images)
➡️ Adds Softmax layer → outputs probabilities

Step 13: View predictions
python
Copy
Edit
predictions[0]       # Probabilities of each class for 1st image
np.argmax(predictions[0])  # Predicted class
test_labels[0]       # True class
Step 14: Plot prediction + probabilities
Functions to visualize image + confidence bar chart

python
Copy
Edit
def plot_image(i, predictions_array, true_label, img):
    ...
def plot_value_array(i, predictions_array, true_label):
    ...
➡️ Blue = correct prediction
➡️ Red = incorrect prediction

Step 15: Visualize predictions
python
Copy
Edit
i = 0
plt.subplot(1,2,1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1,2,2)
plot_value_array(i, predictions[i], test_labels)
plt.show()
➡️ Shows image + bar chart of predicted probabilities

Step 16: Plot multiple predictions
python
Copy
Edit
for i in range(num_images):
    plot_image(i, predictions[i], ...)
    plot_value_array(i, predictions[i], ...)
➡️ Shows 15 images (5x3 grid) with predicted + true labels

Step 17: Predict single image
python
Copy
Edit
img = test_images[1]
img = np.expand_dims(img,0)  # Shape (1,28,28)
predictions_single = probability_model.predict(img)

np.argmax(predictions_single[0])  # Predicted class
➡️ Predicts class for 1 image

✨ 2️⃣ Key Observations
Observation	Meaning
Simple architecture performs well	Even small neural nets work fine on Fashion MNIST
Overfitting possible with 30 epochs	Training for fewer epochs OR adding validation would help
Softmax layer added post hoc	Easier to interpret model outputs (probabilities)
Visualization functions (blue/red)	Quickly diagnose correct vs incorrect predictions

❓ 3️⃣ Sample Viva / Interview QnA
Basics
Q1: What dataset is used here?
A: Fashion MNIST — 28×28 grayscale images of fashion items (10 classes)

Q2: Why normalize pixel values?
A: To scale inputs to [0,1] → helps model train faster and converge better

Model
Q3: Why use Flatten layer?
A: Converts 2D image (28×28) into 1D vector (784) → needed for Dense layer

Q4: Why Softmax at the end?
A: Converts logits into class probabilities (sums to 1)

Q5: Why SparseCategoricalCrossentropy?
A: Labels are integers (0-9), not one-hot vectors

Evaluation
Q6: How to evaluate model performance?
A: Using accuracy on test set

Q7: What does np.argmax(predictions[0]) return?
A: Predicted class (index with highest probability)

Q8: How can you improve the model?
A:

Add dropout / batch normalization

Increase hidden layers / neurons

Use validation set

Reduce epochs (avoid overfitting)

Advanced
Q9: Why is Softmax added separately (not in model)?
A: During training we use logits (better numerically). For inference, we want probabilities → so we add Softmax after training.

Q10: Can CNNs be used here?
A: Yes — CNNs are better suited for image classification tasks

✅ Summary
This code:

Trains a fully connected network to classify Fashion MNIST images

Visualizes both predictions and confidence scores

Evaluates model accuracy on unseen data



