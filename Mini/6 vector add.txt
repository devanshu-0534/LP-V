 EXPLANATION OF THE CUDA CODE
üîß 1. Kernel Function
cpp
Copy
Edit
__global__ void vectorAdd(int *a, int *b, int *c, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) c[i] = a[i] + b[i];
}
__global__: This marks the function as a kernel, callable from host and executed on device (GPU).

Each thread calculates one element of the output vector c[i] = a[i] + b[i].

blockIdx.x, blockDim.x, and threadIdx.x are CUDA-provided variables to identify unique thread index.

üßÆ 2. Host Code ‚Äì Setup and Memory Allocation
cpp
Copy
Edit
int *a = new int[n];
int *b = new int[n];
int *c = new int[n];
Host memory allocation (CPU).

üì• 3. Input Reading
cpp
Copy
Edit
cin >> a[i];  // and similarly for b
Takes user input for vectors a and b.

üß† 4. Device Memory Allocation
cpp
Copy
Edit
cudaMalloc(&d_a, n * sizeof(int));
Allocates memory on the GPU for a, b, and c.

üöÄ 5. Copy Data to Device
cpp
Copy
Edit
cudaMemcpy(d_a, a, n * sizeof(int), cudaMemcpyHostToDevice);
Copies arrays from CPU to GPU memory.

üßµ 6. Kernel Launch
cpp
Copy
Edit
vectorAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);
Launches gridSize √ó blockSize threads.

Each thread adds a pair of elements a[i] + b[i].

üß† Calculation:

cpp
Copy
Edit
int blockSize = 256;
int gridSize = (n + blockSize - 1) / blockSize;
Ensures that enough threads are created to cover all n elements.

üîÑ 7. Synchronization and Error Check
cpp
Copy
Edit
cudaDeviceSynchronize();
cudaGetLastError();
Waits for GPU to finish, then checks for errors.

üì§ 8. Copy Results Back
cpp
Copy
Edit
cudaMemcpy(c, d_c, n * sizeof(int), cudaMemcpyDeviceToHost);
Brings result vector c back to host memory from GPU.

üßπ 9. Cleanup
cpp
Copy
Edit
cudaFree(...); delete[] ...;
Frees device and host memory.

üîÅ COMPARISON WITH CPU-ONLY VERSION
Feature	CPU Version	CUDA Version (GPU)
Speed	Slower for large arrays	Much faster (parallelism)
Threads	Single	Thousands of GPU threads
Memory	Uses only host RAM	Requires explicit GPU memory
Performance Overhead	None	Kernel launch and copy time

‚ùì IMPORTANT Q&A (CUDA)
1. What is a CUDA kernel?
A CUDA kernel is a function executed on the GPU, with many threads running it in parallel.

2. What does blockIdx.x * blockDim.x + threadIdx.x calculate?
It calculates the global thread ID so that each thread works on a unique element of the array.

3. Why do we check if (i < n) in the kernel?
To avoid out-of-bounds access‚Äîin case the total number of threads exceeds n.

4. What is the purpose of cudaMemcpy()?
Transfers data between host and device memory.

Required because CPU and GPU have separate memory spaces.

5. What happens if you forget cudaDeviceSynchronize()?
The CPU may proceed before the GPU finishes, leading to incorrect or incomplete results.

6. Why is memory freed with cudaFree() and delete[]?
cudaFree() for GPU memory (d_a, d_b, d_c)

delete[] for CPU memory (a, b, c)

7. What are <<<gridSize, blockSize>>>?
They define the execution configuration:

gridSize: Number of thread blocks.

blockSize: Threads per block.
This setup lets CUDA launch gridSize √ó blockSize threads.

